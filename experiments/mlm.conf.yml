model:
    name: bal_bert
    args:  # these are args are from transformers/models/bert/configuration_bert.py
        vocab_size: 52_000
        max_position_embeddings: 512
        hidden_size: 100 #512
        num_attention_heads: 5
        num_hidden_layers: 2
        type_vocab_size: 1
        intermediate_size: 200 # 2048
        hidden_act: gelu
        hidden_dropout_prob: 0.1
        attention_probs_dropout_prob: 0.1

        initializer_range: 0.02
        layer_norm_eps: 1e-12
        pad_token_id: 0
        position_embedding_typ: absolute  #relative_key relative_key_query
        gradient_checkpointing: False   # use gradient checkpointing to save memory at the expense of slower backward pass.

optimizer:
    name: adam
    args:
        lr: 0.0005
        betas: [0.9, 0.999]
schedule:
    name: inverse_sqrt
    args:
        peak_lr: 0.0005
        warmup: 100
loss:
    name: cross_entropy
    args:
        #weight_by: inverse_frequency   #others: inverse_log, inverse_sqrt, information_content
prep:
    #vocab: bpe
    vocab_size: 8000
    min_frequency: 2
    max_length: 512     # truncate seqs after this


train:
    data: data/mlm/cc_news-train.dedup.tok.1M
    batch_size: 5 #10
    #max_step: 10_000
    #max_step: 300
    max_epoch: 100
    # checkpoint: 1_000
    checkpoint: 100

validation:
    data: data/mlm/cc_news-train.dedup.tok.1K
    batch_size: 10
    patience: 10
    by: macro_f1
    keep: 10                  # keep these many models

tests:
    #test: data/msl/test       # dont use tests
    val: data/msl/val





